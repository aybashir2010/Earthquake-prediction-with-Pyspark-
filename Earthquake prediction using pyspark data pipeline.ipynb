{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6cc4135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091007a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# configuring spark session\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local[2]')\\\n",
    "    .appName('quake_etl')\\\n",
    "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:2.4.1')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f006b07d",
   "metadata": {},
   "source": [
    "# SECTION 1: Data processing with Pyspark and MongoDB\n",
    "### Data Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d721569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.take of DataFrame[Date: string, Time: string, Latitude: string, Longitude: string, Type: string, Depth: string, Depth Error: string, Depth Seismic Stations: string, Magnitude: string, Magnitude Type: string, Magnitude Error: string, Magnitude Seismic Stations: string, Azimuthal Gap: string, Horizontal Distance: string, Horizontal Error: string, Root Mean Square: string, ID: string, Source: string, Location Source: string, Magnitude Source: string, Status: string]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df_load =spark.read.csv(r\"C:\\Users\\user\\Desktop\\DataScience\\EarthQuake\\database.csv\", header=True)\n",
    "# preview df_load\n",
    "df_load.take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9325ee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|        6|            MW|ISCGEM860706|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake|   80|      5.8|            MW|ISCGEM860737|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake|   20|      6.2|            MW|ISCGEM860762|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake|   15|      5.8|            MW|ISCGEM860856|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake|   15|      5.8|            MW|ISCGEM860890|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Droping field we dont need from the dataframe\n",
    "lst_dropped_columns = ['Depth Error','Depth Seismic Stations','Magnitude Error','Magnitude Seismic Stations','Azimuthal Gap',\n",
    "                      'Horizontal Distance','Horizontal Error','Root Mean Square','Source','Location Source',\n",
    "                       'Magnitude Source','Status','Time']\n",
    "df_load = df_load.drop(*lst_dropped_columns)\n",
    "# preview df_load\n",
    "df_load.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfd6552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|Year|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|        6|            MW|ISCGEM860706|1965|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake|   80|      5.8|            MW|ISCGEM860737|1965|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake|   20|      6.2|            MW|ISCGEM860762|1965|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake|   15|      5.8|            MW|ISCGEM860856|1965|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake|   15|      5.8|            MW|ISCGEM860890|1965|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating a year field and adding to the dataframe\n",
    "df_load =df_load.withColumn('Year', year(to_timestamp('Date', 'dd/MM/yyyy')))\n",
    "# Prreview df_load\n",
    "df_load.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc498109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|Year|Counts|\n",
      "+----+------+\n",
      "|1990|   196|\n",
      "|1975|   150|\n",
      "|1977|   148|\n",
      "|2003|   187|\n",
      "|2007|   211|\n",
      "+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the quake frequency dataframe using the year field and counts for each year\n",
    "df_quake_freq = df_load.groupBy('Year').count().withColumnRenamed('count', 'Counts')\n",
    "df_quake_freq.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a90e3",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de364bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Depth: string (nullable = true)\n",
      " |-- Magnitude: string (nullable = true)\n",
      " |-- Magnitude Type: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preview df_load schema\n",
    "df_load.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31436ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|Year|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|      6.0|            MW|ISCGEM860706|1965|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake| 80.0|      5.8|            MW|ISCGEM860737|1965|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake| 20.0|      6.2|            MW|ISCGEM860762|1965|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake| 15.0|      5.8|            MW|ISCGEM860856|1965|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake| 15.0|      5.8|            MW|ISCGEM860890|1965|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cast some field from string into numeric types\n",
    "df_load = df_load.withColumn('Latitude', df_load['Latitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Longitude', df_load['Longitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Depth', df_load['Depth'].cast(DoubleType()))\\\n",
    "    .withColumn('Magnitude', df_load['Magnitude'].cast(DoubleType()))\\\n",
    "\n",
    "# Preview dataframe again\n",
    "df_load.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ac50e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Depth: double (nullable = true)\n",
      " |-- Magnitude: double (nullable = true)\n",
      " |-- Magnitude Type: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview df_load Schema again\n",
    "df_load.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbbb5d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|Year|    Avg_Magnitude|\n",
      "+----+-----------------+\n",
      "|1990|5.858163265306125|\n",
      "|1975| 5.84866666666667|\n",
      "|1977|5.757432432432437|\n",
      "|2003|5.850802139037435|\n",
      "|2007| 5.89099526066351|\n",
      "+----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating average magnitude and maximum magnitude for the quake frequency\n",
    "df_max = df_load.groupBy('Year').max('Magnitude').withColumnRenamed('max(Magnitude)','Max_Magnitude')\n",
    "df_avg = df_load.groupBy('Year').avg('Magnitude').withColumnRenamed('avg(Magnitude)','Avg_Magnitude')\n",
    "\n",
    "df_avg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6ec4e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------------+-------------+\n",
      "|Year|Counts|    Avg_Magnitude|Max_Magnitude|\n",
      "+----+------+-----------------+-------------+\n",
      "|1990|   196|5.858163265306125|          7.6|\n",
      "|1975|   150| 5.84866666666667|          7.8|\n",
      "|1977|   148|5.757432432432437|          7.6|\n",
      "|2003|   187|5.850802139037435|          7.6|\n",
      "|2007|   211| 5.89099526066351|          8.4|\n",
      "+----+------+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join df_max and df_avg to df_quake_freq\n",
    "df_quake_freq = df_quake_freq.join(df_avg, ['Year']).join(df_max,['Year'])\n",
    "# Preview df_quake_freq\n",
    "df_quake_freq.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9479b2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: int, Counts: bigint, Avg_Magnitude: double, Max_Magnitude: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove nulls\n",
    "df_load.dropna()\n",
    "df_quake_freq.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47493828",
   "metadata": {},
   "source": [
    "## Data Loading into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fc93505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the tables/collections in mongodb\n",
    "# Write df_load to mongodb\n",
    "df_load.write.format('mongo')\\\n",
    "     .mode('overwrite')\\\n",
    "     .option('spark.mongodb.output.uri','mongodb://127.0.0.1:27017/Quake.quakes').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4b8a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write df_quake_freq to mongodb\n",
    "df_quake_freq.write.format('mongo')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('spark.mongodb.output.uri','mongodb://127.0.0.1:27017/Quake.quake_freq').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe873f5",
   "metadata": {},
   "source": [
    "# SECTION 2:Machine Learning with pyspark and MongoDB\n",
    "\n",
    "## Data Pre-processing and building the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7b9d924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(time='2017-01-02T00:13:06.300Z', latitude='-36.0365', longitude='51.9288', depth='10', mag='5.7', magType='mwb', nst=None, gap='26', dmin='14.685', rms='1.37', net='us', id='us10007p5d', updated='2017-03-27T23:53:17.040Z', place='Southwest Indian Ridge', type='earthquake', horizontalError='10.3', depthError='1.7', magError='0.068', magNst='21', status='reviewed', locationSource='us', magSource='us')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the test data file into a dataframe\n",
    "df_test = spark.read.csv(r\"C:\\Users\\user\\Desktop\\DataScience\\EarthQuake\\query.csv\", header=True)\n",
    "# preview df_load\n",
    "df_test.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "188572f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------+--------+---------+---------+--------------+----------+----+--------------------+\n",
      "|      Date|Depth|          ID|Latitude|Longitude|Magnitude|Magnitude Type|      Type|Year|                 _id|\n",
      "+----------+-----+------------+--------+---------+---------+--------------+----------+----+--------------------+\n",
      "|01/02/1965|131.6|ISCGEM860706|  19.246|  145.616|      6.0|            MW|Earthquake|1965|{634595629fd41d24...|\n",
      "|01/04/1965| 80.0|ISCGEM860737|   1.863|  127.352|      5.8|            MW|Earthquake|1965|{634595629fd41d24...|\n",
      "|01/05/1965| 20.0|ISCGEM860762| -20.579| -173.972|      6.2|            MW|Earthquake|1965|{634595629fd41d24...|\n",
      "|01/08/1965| 15.0|ISCGEM860856| -59.076|  -23.557|      5.8|            MW|Earthquake|1965|{634595629fd41d24...|\n",
      "|01/09/1965| 15.0|ISCGEM860890|  11.938|  126.427|      5.8|            MW|Earthquake|1965|{634595629fd41d24...|\n",
      "+----------+-----+------------+--------+---------+---------+--------------+----------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the training data from mongo into a dataframe\n",
    "df_train = spark.read.format('mongo')\\\n",
    "    .option('spark.mongodb.input.uri','mongodb://127.0.0.1:27017/Quake.quakes').load()\n",
    "\n",
    "# preview train dataset\n",
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bebbcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+---+------+\n",
      "|                time|latitude|longitude|mag| depth|\n",
      "+--------------------+--------+---------+---+------+\n",
      "|2017-01-02T00:13:...|-36.0365|  51.9288|5.7|    10|\n",
      "|2017-01-02T13:13:...|  -4.895| -76.3675|5.9|   106|\n",
      "|2017-01-02T13:14:...|-23.2513| 179.2383|6.3|551.62|\n",
      "|2017-01-03T09:09:...| 24.0151|  92.0177|5.7|    32|\n",
      "|2017-01-03T21:19:...|-43.3527| -74.5017|5.5| 10.26|\n",
      "+--------------------+--------+---------+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleaning of testing data and remove fields we dont need\n",
    "df_test_clean = df_test['time','latitude','longitude','mag','depth']\n",
    "df_test_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e634707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+---------+------+\n",
      "|                Date|Latitude|Longitude|Magnitude| Depth|\n",
      "+--------------------+--------+---------+---------+------+\n",
      "|2017-01-02T00:13:...|-36.0365|  51.9288|      5.7|    10|\n",
      "|2017-01-02T13:13:...|  -4.895| -76.3675|      5.9|   106|\n",
      "|2017-01-02T13:14:...|-23.2513| 179.2383|      6.3|551.62|\n",
      "|2017-01-03T09:09:...| 24.0151|  92.0177|      5.7|    32|\n",
      "|2017-01-03T21:19:...|-43.3527| -74.5017|      5.5| 10.26|\n",
      "+--------------------+--------+---------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming the test dataset field to match our training data\n",
    "df_test_clean = df_test_clean.withColumnRenamed('time','Date')\\\n",
    "    .withColumnRenamed('latitude','Latitude')\\\n",
    "    .withColumnRenamed('longitude','Longitude')\\\n",
    "    .withColumnRenamed('mag','Magnitude')\\\n",
    "    .withColumnRenamed('depth','Depth')\\\n",
    "\n",
    "# previewing the dataframe again\n",
    "df_test_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bba516e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Magnitude: string (nullable = true)\n",
      " |-- Depth: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preview schema of test dataset\n",
    "df_test_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c16e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the data type of some variables into numerical data type\n",
    "df_test_clean = df_test_clean.withColumn('Latitude', df_test_clean['Latitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Longitude', df_test_clean['Longitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Depth', df_test_clean['Depth'].cast(DoubleType()))\\\n",
    "    .withColumn('Magnitude', df_test_clean['Magnitude'].cast(DoubleType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c34904a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Magnitude: double (nullable = true)\n",
      " |-- Depth: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c55b1362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing dataframes\n",
    "df_testing = df_test_clean['Latitude','Longitude','Magnitude', 'Depth']\n",
    "df_training = df_train['Latitude','Longitude','Magnitude', 'Depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39d0a03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+-----+\n",
      "|Latitude|Longitude|Magnitude|Depth|\n",
      "+--------+---------+---------+-----+\n",
      "|  19.246|  145.616|      6.0|131.6|\n",
      "|   1.863|  127.352|      5.8| 80.0|\n",
      "| -20.579| -173.972|      6.2| 20.0|\n",
      "| -59.076|  -23.557|      5.8| 15.0|\n",
      "|  11.938|  126.427|      5.8| 15.0|\n",
      "+--------+---------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23412"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview df_training \n",
    "df_training.show(5)\n",
    "df_training.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cdfb7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------+\n",
      "|Latitude|Longitude|Magnitude| Depth|\n",
      "+--------+---------+---------+------+\n",
      "|-36.0365|  51.9288|      5.7|  10.0|\n",
      "|  -4.895| -76.3675|      5.9| 106.0|\n",
      "|-23.2513| 179.2383|      6.3|551.62|\n",
      "| 24.0151|  92.0177|      5.7|  32.0|\n",
      "|-43.3527| -74.5017|      5.5| 10.26|\n",
      "+--------+---------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "397"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview df_testing\n",
    "df_testing.show(5)\n",
    "df_testing.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8264c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop records with null values from our dataframes\n",
    "df_training = df_training.dropna()\n",
    "df_testing = df_testing.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e859940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e0854fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features to parse into our model and then create the feature vector\n",
    "assembler = VectorAssembler(inputCols=['Latitude','Longitude','Depth'], outputCol='features')\n",
    "\n",
    "# create the model\n",
    "model_reg = RandomForestRegressor(featuresCol='features', labelCol='Magnitude')\n",
    "\n",
    "# chain the assembler with the model in a pipeline\n",
    "pipeline = Pipeline(stages=[assembler, model_reg])\n",
    "\n",
    "#Train the model\n",
    "model = pipeline.fit(df_training)\n",
    "\n",
    "# Make the prediction\n",
    "pred_results = model.transform(df_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71800b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------+--------------------+-----------------+\n",
      "|Latitude|Longitude|Magnitude| Depth|            features|       prediction|\n",
      "+--------+---------+---------+------+--------------------+-----------------+\n",
      "|-36.0365|  51.9288|      5.7|  10.0|[-36.0365,51.9288...|5.827504771162459|\n",
      "|  -4.895| -76.3675|      5.9| 106.0|[-4.895,-76.3675,...|5.894648370207244|\n",
      "|-23.2513| 179.2383|      6.3|551.62|[-23.2513,179.238...|5.898069295971082|\n",
      "| 24.0151|  92.0177|      5.7|  32.0|[24.0151,92.0177,...|5.888635235631126|\n",
      "|-43.3527| -74.5017|      5.5| 10.26|[-43.3527,-74.501...|5.966163600421247|\n",
      "+--------+---------+---------+------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview pred_results dataframe\n",
    "pred_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8a58d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square (RMSE) on test data = 0.403477\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of the model\n",
    "# rmse should be less than 0.5 for the model to be useful\n",
    "evaluator = RegressionEvaluator(labelCol='Magnitude', predictionCol='prediction', metricName ='rmse')\n",
    "rmse = evaluator.evaluate(pred_results)\n",
    "print('Root Mean Square (RMSE) on test data = %g' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443cca45",
   "metadata": {},
   "source": [
    "## Prediction Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd35abe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------------+----+------------------+\n",
      "|Latitude|Longitude|   Pred_Magnitude|Year|              RMSE|\n",
      "+--------+---------+-----------------+----+------------------+\n",
      "|-36.0365|  51.9288|5.827504771162459|2017|0.4034772789700646|\n",
      "|  -4.895| -76.3675|5.894648370207244|2017|0.4034772789700646|\n",
      "|-23.2513| 179.2383|5.898069295971082|2017|0.4034772789700646|\n",
      "| 24.0151|  92.0177|5.888635235631126|2017|0.4034772789700646|\n",
      "|-43.3527| -74.5017|5.966163600421247|2017|0.4034772789700646|\n",
      "+--------+---------+-----------------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the prediction dataset\n",
    "df_pred_results = pred_results['Latitude','Longitude', 'prediction']\n",
    "\n",
    "#Rename the prediction field\n",
    "df_pred_results = df_pred_results.withColumnRenamed('prediction', 'Pred_Magnitude')\n",
    "\n",
    "# Add more columns to our prediction dataset\n",
    "df_pred_results = df_pred_results.withColumn('Year', lit(2017))\\\n",
    "    .withColumn('RMSE', lit(rmse))\n",
    "# Preview df_pred_results\n",
    "df_pred_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10720bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prediction dataset into mongo db\n",
    "# write df_pred_results\n",
    "df_pred_results.write.format('mongo')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('spark.mongodb.output.uri', 'mongodb://127.0.0.1:27017/Quake.pred_results').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef13631b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8f12f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739f545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8a5371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20844355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b2a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d85cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
